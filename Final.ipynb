{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinha\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jinha\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['NVDA']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) HuggingFace / FinBERT imports\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 2) Scikit‐learn for regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yfinance as yf\n",
    "\n",
    "# ============================================\n",
    "#  STEP A: LOAD RAW DATA\n",
    "# ============================================\n",
    "\n",
    "# ---- A.1: News CSV ----\n",
    "# Assumed format: \"news_2022.csv\" with columns:\n",
    "#    Date (e.g. \"2022-01-03 09:15:00 UTC\"), \n",
    "#    Article_title (headline string)\n",
    "news_df = pd.read_csv(\"nasdaq_2022.csv\", parse_dates=[\"Date\"])\n",
    "news_df.rename(columns={\"Date\": \"timestamp\", \"Article_title\": \"headline\"}, inplace=True)\n",
    "\n",
    "# If there is a 'Ticker' column in your CSV, you can keep it. \n",
    "# For simplicity below, we assume all headlines refer to **one** ticker. If you have multiple tickers,\n",
    "# you can add a \"Ticker\" column and then groupby ticker as shown later.\n",
    "\n",
    "# Extract just the calendar date for grouping (UTC → date)\n",
    "news_df[\"date\"] = news_df[\"timestamp\"].dt.date\n",
    "\n",
    "# ---- A.2: Returns CSV ----\n",
    "# Assumed format: \"returns_2022.csv\" with at least columns:\n",
    "#    Date (e.g. \"2022-01-03\"),\n",
    "#    Close (float)\n",
    "# Download daily adjusted close prices for Google (GOOG) for 2022\n",
    "goog = yf.download(\"GOOG\", start=\"2022-01-01\", end=\"2023-01-01\", progress=False)\n",
    "ret_df = goog.reset_index()[[\"Date\", \"Close\"]]\n",
    "ret_df.rename(columns={\"Date\": \"date\", \"Close\": \"close\"}, inplace=True)\n",
    "ret_df[\"date\"] = ret_df[\"date\"].dt.date\n",
    "\n",
    "# Compute next‐day return per date. We'll align \"news on D\" → \"return on D+1\".\n",
    "ret_df[\"next_day_return\"] = (\n",
    "    ret_df[\"close\"].shift(-1) - ret_df[\"close\"]\n",
    ") / ret_df[\"close\"]\n",
    "\n",
    "# Drop the last row (it has no next‐day price)\n",
    "ret_df = ret_df.iloc[:-1].copy()\n",
    "\n",
    "# ============================================\n",
    "#  STEP B: SET UP EMBEDDERS & FINBERT\n",
    "# ============================================\n",
    "\n",
    "# ---- B.1: Sentence‐Transformer for relevance ----\n",
    "# We'll load a pretrained sentence‐transformer (e.g. all‐MPNet‐base‐v2) to embed headlines + a ticker description.\n",
    "# If you prefer a financial‐domain SBERT (e.g. \"patrickvonplaten/finbert-sentiment\" or any custom model),\n",
    "# just swap the model name below.\n",
    "sbert_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Pre‐compute an embedding for your stock description. For example:\n",
    "stock_description = (\n",
    "    \"Alphabet Inc. (Google) is a global technology company specializing in internet-related services and products, including search, advertising, cloud computing, software, and hardware. \"\n",
    "    \"Its core products include Google Search, YouTube, Android, Google Cloud, and more.\"\n",
    ")\n",
    "stock_desc_emb = sbert_model.encode(stock_description, convert_to_tensor=True)\n",
    "# If you have multiple tickers, you could do a dict { \"AAPL\": embAAPL, \"TSLA\": embTSLA, ... }.\n",
    "\n",
    "# ---- B.2: FinBERT sentiment pipeline ----\n",
    "# We will load the FinBERT model from HuggingFace. Note: this may download ~400 MB the first time.\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "finbert_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=finbert_model,\n",
    "    tokenizer=finbert_tokenizer,\n",
    "    device=0  # Use GPU (cuda:0)\n",
    ")\n",
    "\n",
    "# Helper: take the pipeline output and return (P_pos, P_neg, P_neu)\n",
    "def finbert_probs_for_text(text: str):\n",
    "    \"\"\"\n",
    "    Returns a tuple (P_pos, P_neg, P_neu) for the given headline text.\n",
    "    The FinBERT pipeline returns something like:\n",
    "      [ {\"label\":\"Positive\",\"score\":0.897}, {\"label\":\"Neutral\",\"score\":0.070}, {\"label\":\"Negative\",\"score\":0.033} ]\n",
    "    We reorder or match appropriately.\n",
    "    \"\"\"\n",
    "    raw = finbert_pipeline(text, truncation=True, max_length=128)\n",
    "    # The raw output may be a list of 3 dicts; we map them into a dict label→score\n",
    "    probs = { d[\"label\"].lower(): d[\"score\"] for d in raw }\n",
    "    # Ensure all three keys exist; if missing, set to 0:\n",
    "    Ppos = probs.get(\"positive\", 0.0)\n",
    "    Pneu = probs.get(\"neutral\", 0.0)\n",
    "    Pneg = probs.get(\"negative\", 0.0)\n",
    "    return Ppos, Pneg, Pneu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb5a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005f3b4344454e0ea069ce11406a18e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280354/280354 [53:48<00:00, 86.83it/s] \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#  STEP C: PROCESS EACH ARTICLE\n",
    "# ============================================\n",
    "\n",
    "# We'll build three new columns in news_df:\n",
    "#   1) r_i = ## relevance (cosine between stock_desc_emb and headline_emb)\n",
    "#   2) P_pos, P_neg, P_neu (from FinBERT)\n",
    "#   3) sentiment_scalar = P_pos - P_neg\n",
    "#   4) feat_pos = r_i * P_pos, feat_neg = r_i * P_neg, feat_neu = r_i * P_neu\n",
    "\n",
    "relevance_list = []\n",
    "Ppos_list = []\n",
    "Pneg_list = []\n",
    "Pneu_list = []\n",
    "sent_scalar_list = []\n",
    "feat_pos_list = []\n",
    "feat_neg_list = []\n",
    "feat_neu_list = []\n",
    "\n",
    "# For speed, do all headline embeddings in batch:\n",
    "all_headlines = news_df[\"headline\"].tolist()\n",
    "headline_embs = sbert_model.encode(all_headlines, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "for idx, row in tqdm(news_df.iterrows(), total=len(news_df)):\n",
    "    h_emb = headline_embs[idx]  # torch.Tensor\n",
    "    # 1) relevance = cosine(stock_desc_emb, headline_embs[idx])\n",
    "    cos_sim = util.cos_sim(stock_desc_emb, h_emb).item()\n",
    "    relevance_list.append(cos_sim)\n",
    "\n",
    "    # 2) FinBERT probabilities:\n",
    "    Ppos, Pneg, Pneu = finbert_probs_for_text(row[\"headline\"])\n",
    "    Ppos_list.append(Ppos)\n",
    "    Pneg_list.append(Pneg)\n",
    "    Pneu_list.append(Pneu)\n",
    "\n",
    "    # 3) sentiment_scalar = P_pos - P_neg\n",
    "    s_scalar = Ppos - Pneg\n",
    "    sent_scalar_list.append(s_scalar)\n",
    "\n",
    "    # 4) features for regression\n",
    "    feat_pos_list.append(cos_sim * Ppos)\n",
    "    feat_neg_list.append(cos_sim * Pneg)\n",
    "    feat_neu_list.append(cos_sim * Pneu)\n",
    "\n",
    "# Attach new columns\n",
    "news_df[\"relevance\"] = relevance_list\n",
    "news_df[\"P_pos\"] = Ppos_list\n",
    "news_df[\"P_neg\"] = Pneg_list\n",
    "news_df[\"P_neu\"] = Pneu_list\n",
    "news_df[\"sentiment_scalar\"] = sent_scalar_list\n",
    "news_df[\"feat_pos\"] = feat_pos_list\n",
    "news_df[\"feat_neg\"] = feat_neg_list\n",
    "news_df[\"feat_neu\"] = feat_neu_list\n",
    "\n",
    "# (Optional) Drop very low‐relevance articles (e.g. r_i < 0.2)\n",
    "relevance_threshold = 0.2\n",
    "news_df = news_df[news_df[\"relevance\"] >= relevance_threshold].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d967c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## Save all relevant lists to disk using pickle\n",
    "#with open(\"relevance_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(relevance_list, f)\n",
    "#\n",
    "#with open(\"Ppos_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(Ppos_list, f)\n",
    "#\n",
    "#with open(\"Pneg_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(Pneg_list, f)\n",
    "#\n",
    "#with open(\"Pneu_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(Pneu_list, f)\n",
    "#\n",
    "#with open(\"sent_scalar_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(sent_scalar_list, f)\n",
    "#\n",
    "#with open(\"feat_pos_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(feat_pos_list, f)\n",
    "#\n",
    "#with open(\"feat_neg_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(feat_neg_list, f)\n",
    "#\n",
    "#with open(\"feat_neu_list.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(feat_neu_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5861b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"relevance_list.pkl\", \"rb\") as f:\n",
    "    relevance_list = pickle.load(f)\n",
    "\n",
    "with open(\"Ppos_list.pkl\", \"rb\") as f:\n",
    "    Ppos_list = pickle.load(f)\n",
    "\n",
    "with open(\"Pneg_list.pkl\", \"rb\") as f:\n",
    "    Pneg_list = pickle.load(f)\n",
    "\n",
    "with open(\"Pneu_list.pkl\", \"rb\") as f:\n",
    "    Pneu_list = pickle.load(f)\n",
    "\n",
    "with open(\"sent_scalar_list.pkl\", \"rb\") as f:\n",
    "    sent_scalar_list = pickle.load(f)\n",
    "\n",
    "with open(\"feat_pos_list.pkl\", \"rb\") as f:\n",
    "    feat_pos_list = pickle.load(f)\n",
    "\n",
    "with open(\"feat_neg_list.pkl\", \"rb\") as f:\n",
    "    feat_neg_list = pickle.load(f)\n",
    "\n",
    "with open(\"feat_neu_list.pkl\", \"rb\") as f:\n",
    "    feat_neu_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0cd0db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_16092\\2751305670.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_raw  = grouped.apply(lambda df: np.sum(df[\"relevance\"] * df[\"sentiment_scalar\"])).rename(\"RawSentiment\")\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#  STEP D: AGGREGATE TO DAILY “NormSentiment”\n",
    "# ============================================\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Ensure 'relevance' and 'sentiment_scalar' columns exist in news_df\n",
    "if \"relevance\" not in news_df.columns or \"sentiment_scalar\" not in news_df.columns:\n",
    "    news_df = news_df.copy()\n",
    "    news_df[\"relevance\"] = relevance_list\n",
    "    news_df[\"sentiment_scalar\"] = sent_scalar_list\n",
    "\n",
    "# 1) Compute RawSentiment_D = sum_i (r_i * (P_pos_i - P_neg_i)) per date\n",
    "# 2) Compute TotalRelevance_D = sum_i (r_i) per date\n",
    "# 3) NormSentiment_D = RawSentiment_D / (TotalRelevance_D + ε)\n",
    "\n",
    "grouped = news_df.groupby(\"date\")\n",
    "daily_raw  = grouped.apply(lambda df: np.sum(df[\"relevance\"] * df[\"sentiment_scalar\"])).rename(\"RawSentiment\")\n",
    "daily_rel  = grouped[\"relevance\"].sum().rename(\"TotalRelevance\")\n",
    "\n",
    "daily_sentiment_df = pd.concat([daily_raw, daily_rel], axis=1).reset_index()\n",
    "daily_sentiment_df[\"NormSentiment\"] = (\n",
    "    daily_sentiment_df[\"RawSentiment\"] / (daily_sentiment_df[\"TotalRelevance\"] + epsilon)\n",
    ")\n",
    "\n",
    "# Now `daily_sentiment_df` has columns:\n",
    "#    date | RawSentiment | TotalRelevance | NormSentiment\n",
    "\n",
    "# ============================================\n",
    "#  STEP E: MERGE DAILY SENTIMENT WITH RETURNS\n",
    "# ============================================\n",
    "\n",
    "# Our ret_df has columns [date, close, next_day_return].\n",
    "# We want to align “news on date D” → “return on date D+1”.\n",
    "# We already computed ret_df[\"next_day_return\"] via shift(-1). \n",
    "# So we can merge on `date`=D between daily_sentiment_df (signal on D) and ret_df (return on D).\n",
    "\n",
    "# Flatten ret_df columns if they are MultiIndex\n",
    "if isinstance(ret_df.columns, pd.MultiIndex):\n",
    "    ret_df.columns = ['_'.join([str(i) for i in col if i]).strip('_') for col in ret_df.columns.values]\n",
    "\n",
    "merged_daily = pd.merge(\n",
    "    left = daily_sentiment_df,\n",
    "    right = ret_df[[\"date\", \"next_day_return\"]],\n",
    "    on = \"date\",\n",
    "    how = \"inner\"\n",
    ")\n",
    "\n",
    "# merged_daily now has: date | RawSentiment | TotalRelevance | NormSentiment | next_day_return\n",
    "# Save daily_sentiment_df to CSV\n",
    "daily_sentiment_df.to_csv(\"daily_sentiment_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7fc1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Learned coefficients for f(r,P) ===\n",
      "  feature  coefficient\n",
      "r * P_pos    -0.003063\n",
      "r * P_neg     0.002731\n",
      "r * P_neu    -0.002993\n",
      "\n",
      "Test R^2 score = 0.0000\n",
      "\n",
      "=== Sample of daily_sentiment_df ===\n",
      "      date  RawSentiment  TotalRelevance  NormSentiment\n",
      "2022-01-01     -0.663641        8.145452      -0.081474\n",
      "2022-01-02      0.202113       15.495500       0.013043\n",
      "2022-01-03      7.190523       68.578552       0.104851\n",
      "2022-01-04      4.512509       72.306265       0.062408\n",
      "2022-01-05      4.750061       73.277095       0.064823\n",
      "\n",
      "=== Sample of merged_daily (NormSentiment vs. next_day_return) ===\n",
      "      date  RawSentiment  TotalRelevance  NormSentiment  next_day_return\n",
      "2022-01-03      7.190523       68.578552       0.104851        -0.004536\n",
      "2022-01-04      4.512509       72.306265       0.062408        -0.046830\n",
      "2022-01-05      4.750061       73.277095       0.064823        -0.000745\n",
      "2022-01-06      6.030978       74.090826       0.081400        -0.003973\n",
      "2022-01-07      0.339609       75.945931       0.004472         0.011456\n",
      "\n",
      "=== Learned f coefficients ===\n",
      "  feature  coefficient\n",
      "r * P_pos    -0.003063\n",
      "r * P_neg     0.002731\n",
      "r * P_neu    -0.002993\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#  STEP F: LEARN f VIA RIDGE REGRESSION\n",
    "# ============================================\n",
    "\n",
    "# We want to train on **article‐level** data. That is,\n",
    "# each row is a single article_i, with (feat_pos, feat_neg, feat_neu)  → realized next_day_return_i.\n",
    "# But we only know the next‐day return at the daily level. \n",
    "# So we attach each article_i (with `date`=D) to ret(D+1). Since we already computed ret_df[\"next_day_return\"],\n",
    "# we do an article‐level merge:\n",
    "# Ensure feat_pos, feat_neg, feat_neu columns exist in news_df\n",
    "if not all(col in news_df.columns for col in [\"feat_pos\", \"feat_neg\", \"feat_neu\"]):\n",
    "    news_df = news_df.copy()\n",
    "    news_df[\"feat_pos\"] = feat_pos_list\n",
    "    news_df[\"feat_neg\"] = feat_neg_list\n",
    "    news_df[\"feat_neu\"] = feat_neu_list\n",
    "\n",
    "articles_with_returns = pd.merge(\n",
    "    left = news_df,\n",
    "    right = ret_df[[\"date\", \"next_day_return\"]],\n",
    "    on = \"date\",\n",
    "    how = \"inner\"\n",
    ")\n",
    "# Now each article_i has three features: feat_pos_i, feat_neg_i, feat_neu_i, \n",
    "# and label = next_day_return (same for all articles on date D).\n",
    "\n",
    "X = articles_with_returns[[\"feat_pos\", \"feat_neg\", \"feat_neu\"]]\n",
    "y = articles_with_returns[\"next_day_return\"]\n",
    "\n",
    "# Split into train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Extract learned coefficients\n",
    "coeffs_df = pd.DataFrame({\n",
    "    \"feature\": [\"r * P_pos\", \"r * P_neg\", \"r * P_neu\"],\n",
    "    \"coefficient\": ridge.coef_\n",
    "})\n",
    "\n",
    "print(\"\\n=== Learned coefficients for f(r,P) ===\")\n",
    "print(coeffs_df.to_string(index=False))\n",
    "print(f\"\\nTest R^2 score = {ridge.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# ============================================\n",
    "#  STEP G: HOW TO USE THE LEARNED f GOING FORWARD\n",
    "# ============================================\n",
    "\n",
    "w_pos, w_neg, w_neu = ridge.coef_\n",
    "\n",
    "# Suppose you want to compute, for a brand‐new article with (r_new, P_pos_new, P_neg_new, P_neu_new):\n",
    "#       article_score = w_pos * (r_new * P_pos_new)\n",
    "#                     + w_neg * (r_new * P_neg_new)\n",
    "#                     + w_neu * (r_new * P_neu_new)\n",
    "#\n",
    "# Then you can re‐aggregate all article_scores for date D:\n",
    "#   LearnedRaw_D = Σ_i article_score_i \n",
    "# and optionally\n",
    "#   LearnedNorm_D = LearnedRaw_D / ( Σ_i r_i + ε )\n",
    "#\n",
    "# If you store (r, P_pos, P_neg, P_neu) in a DataFrame again, you just do:\n",
    "\n",
    "# Example (pseudocode) if you have a DataFrame `new_articles_df` for “tomorrow’s headlines”:\n",
    "# new_articles_df[\"article_score\"] = (\n",
    "#     w_pos * (new_articles_df[\"relevance\"] * new_articles_df[\"P_pos\"])\n",
    "#   + w_neg * (new_articles_df[\"relevance\"] * new_articles_df[\"P_neg\"])\n",
    "#   + w_neu * (new_articles_df[\"relevance\"] * new_articles_df[\"P_neu\"])\n",
    "# )\n",
    "# Then per date D: \n",
    "# LearnedRaw = new_articles_df.groupby(\"date\")[\"article_score\"].sum()\n",
    "# LearnedNorm = LearnedRaw / (new_articles_df.groupby(\"date\")[\"relevance\"].sum() + epsilon)\n",
    "\n",
    "# ============================================\n",
    "#  STEP H: SUMMARY OUTPUTS\n",
    "# ============================================\n",
    "\n",
    "# 1) daily_sentiment_df  → has NormSentiment per day\n",
    "print(\"\\n=== Sample of daily_sentiment_df ===\")\n",
    "print(daily_sentiment_df.head().to_string(index=False))\n",
    "\n",
    "# 2) merged_daily → (NormSentiment, next_day_return) per day\n",
    "print(\"\\n=== Sample of merged_daily (NormSentiment vs. next_day_return) ===\")\n",
    "print(merged_daily.head().to_string(index=False))\n",
    "\n",
    "# 3) coeffs_df → gives w_pos, w_neg, w_neu\n",
    "print(\"\\n=== Learned f coefficients ===\")\n",
    "print(coeffs_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476bdd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "54 fits failed out of a total of 108.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "38 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      " 0.00063432 0.00063785 0.0006203  0.00062495 0.00059315 0.00060149\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.00141341 0.00144985 0.00132064 0.00135854 0.00122041 0.00126376\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.040734   0.04269037 0.04810679 0.04896945 0.03195852 0.03215282]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "CV R2 (best): 0.048969454771427\n",
      "Test R2 : 0.10973861242436234\n",
      "     feature  importance\n",
      "2  r * P_neu    0.583131\n",
      "0  r * P_pos    0.225721\n",
      "1  r * P_neg    0.191147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# 1) Split data\n",
    "articles = articles_with_returns  # from previous merge\n",
    "X = articles[[\"feat_pos\", \"feat_neg\", \"feat_neu\"]]\n",
    "y = articles[\"next_day_return\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Tune hyperparameters via a small grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [3, 5, None],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"]\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "print(\"Best RF params:\", grid.best_params_)\n",
    "print(\"CV R2 (best):\", grid.best_score_)\n",
    "print(\"Test R2 :\", best_rf.score(X_test, y_test))\n",
    "\n",
    "# 3) Inspect feature importances\n",
    "import pandas as pd\n",
    "importances = best_rf.feature_importances_\n",
    "feat_names = [\"r * P_pos\", \"r * P_neg\", \"r * P_neu\"]\n",
    "feat_imp = pd.DataFrame({\n",
    "    \"feature\": feat_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "print(feat_imp)\n",
    "\n",
    "# 4) (Optional) Evaluate classification accuracy\n",
    "#    You could binarize the target: up = (next_day_return > 0)\n",
    "#    and train RandomForestClassifier on the same features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0866b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Too many splits=5 for number of samples=250 with test_size=100 and gap=0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m\n\u001b[0;32m     12\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m     13\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m     14\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtscv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1252\u001b[0m, in \u001b[0;36mTimeSeriesSplit._split\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of folds=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1250\u001b[0m     )\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m-\u001b[39m gap \u001b[38;5;241m-\u001b[39m (test_size \u001b[38;5;241m*\u001b[39m n_splits) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many splits=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_splits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for number of samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with test_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and gap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1257\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_samples)\n\u001b[0;32m   1258\u001b[0m test_starts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(n_samples \u001b[38;5;241m-\u001b[39m n_splits \u001b[38;5;241m*\u001b[39m test_size, n_samples, test_size)\n",
      "\u001b[1;31mValueError\u001b[0m: Too many splits=5 for number of samples=250 with test_size=100 and gap=0."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = merged_daily[['RawSentiment', 'TotalRelevance', 'NormSentiment']]\n",
    "y = (merged_daily['next_day_return'] > 0).astype(int)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cv_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    cv_scores.append(acc)\n",
    "    \n",
    "    print(\"Fold accuracy:\", round(acc, 4))\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"\\nAverage temporal CV accuracy:\", round(np.mean(cv_scores), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a40bd37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating f1: r*(P_pos-P_neg) ===\n",
      "→ Mean CV accuracy = 0.5463  (± 0.0995)\n",
      "\n",
      "=== Evaluating f2: r*P_pos ===\n",
      "→ Mean CV accuracy = 0.4927  (± 0.0679)\n",
      "\n",
      "=== Evaluating f3: r*(P_pos+0.5P_neu) ===\n",
      "→ Mean CV accuracy = 0.4683  (± 0.0605)\n",
      "\n",
      "=== Evaluating f4: (P_pos-P_neg) ===\n",
      "→ Mean CV accuracy = 0.4976  (± 0.0293)\n",
      "\n",
      "=== Evaluating f5: exp(r*(P_pos-P_neg)) ===\n",
      "→ Mean CV accuracy = 0.4732  (± 0.0396)\n",
      "\n",
      "=== Evaluating f6: r*exp(P_pos-P_neg) ===\n",
      "→ Mean CV accuracy = 0.5268  (± 0.0867)\n",
      "\n",
      "=== Evaluating f7: r*sigmoid(P_pos-P_neg) ===\n",
      "→ Mean CV accuracy = 0.4341  (± 0.0697)\n",
      "\n",
      "=== Evaluating f8: r*(P_pos-P_neg)^2 ===\n",
      "→ Mean CV accuracy = 0.4878  (± 0.0309)\n",
      "\n",
      "=== Evaluating f9: r*(P_pos-P_neg)*(1-P_neu) ===\n",
      "→ Mean CV accuracy = 0.5463  (± 0.0995)\n",
      "\n",
      "=== Summary of Mean CV Accuracies ===\n",
      "f1: r*(P_pos-P_neg)            →  0.5463  ± 0.0995\n",
      "f2: r*P_pos                    →  0.4927  ± 0.0679\n",
      "f3: r*(P_pos+0.5P_neu)         →  0.4683  ± 0.0605\n",
      "f4: (P_pos-P_neg)              →  0.4976  ± 0.0293\n",
      "f5: exp(r*(P_pos-P_neg))       →  0.4732  ± 0.0396\n",
      "f6: r*exp(P_pos-P_neg)         →  0.5268  ± 0.0867\n",
      "f7: r*sigmoid(P_pos-P_neg)     →  0.4341  ± 0.0697\n",
      "f8: r*(P_pos-P_neg)^2          →  0.4878  ± 0.0309\n",
      "f9: r*(P_pos-P_neg)*(1-P_neu)  →  0.5463  ± 0.0995\n",
      "\n",
      "=== Summary of Mean CV F1 Scores ===\n",
      "f1: r*(P_pos-P_neg)            →  0.4762  ± 0.1812\n",
      "f2: r*P_pos                    →  0.4262  ± 0.0561\n",
      "f3: r*(P_pos+0.5P_neu)         →  0.3472  ± 0.1318\n",
      "f4: (P_pos-P_neg)              →  0.4219  ± 0.1050\n",
      "f5: exp(r*(P_pos-P_neg))       →  0.3776  ± 0.0642\n",
      "f6: r*exp(P_pos-P_neg)         →  0.3918  ± 0.1692\n",
      "f7: r*sigmoid(P_pos-P_neg)     →  0.3341  ± 0.0894\n",
      "f8: r*(P_pos-P_neg)^2          →  0.4165  ± 0.0801\n",
      "f9: r*(P_pos-P_neg)*(1-P_neu)  →  0.4762  ± 0.1812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ASSUMPTIONS:\n",
    "# 1) You already have `news_df` with columns:\n",
    "#     ['date' (datetime.date),\n",
    "#      'relevance' (float),\n",
    "#      'P_pos', 'P_neg', 'P_neu' (floats)]\n",
    "#\n",
    "# 2) You already have `returns_df` with columns:\n",
    "#     ['date' (datetime.date),\n",
    "#      'next_day_return' (float)]\n",
    "#\n",
    "# 3) `news_df['date']` maps exactly to the same \"date\" field in `returns_df`.\n",
    "# ------------------------------------------------------------------------------\n",
    "# Example placeholders (remove these if you already loaded your data):\n",
    "# news_df = pd.read_csv(\"news_processed.csv\", parse_dates=[\"date\"])\n",
    "# returns_df = pd.read_csv(\"returns_processed.csv\", parse_dates=[\"date\"])\n",
    "# returns_df[\"next_day_return\"] = (returns_df[\"close\"].shift(-1) - returns_df[\"close\"]) / returns_df[\"close\"]\n",
    "# returns_df = returns_df.iloc[:-1]  # drop last row (no next‐day return)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) Define a bunch of candidate f() functions\n",
    "#    Each function takes a row of news_df (with r, P_pos, P_neg, P_neu) and returns a float\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def f1(r, ppos, pneg, pneu):\n",
    "    # Classic linear: r * (P_pos - P_neg)\n",
    "    return r * (ppos - pneg)\n",
    "\n",
    "def f2(r, ppos, pneg, pneu):\n",
    "    # Only positive‐weighted by relevance\n",
    "    return r * ppos\n",
    "\n",
    "def f3(r, ppos, pneg, pneu):\n",
    "    # Mix positive and neutral as a boost: r * (P_pos + 0.5 * P_neu)\n",
    "    return r * (ppos + 0.5 * pneu)\n",
    "\n",
    "def f4(r, ppos, pneg, pneu):\n",
    "    # Drop relevance entirely, just “pos-neg”\n",
    "    return (ppos - pneg)\n",
    "\n",
    "def f5(r, ppos, pneg, pneu):\n",
    "    # Exponential of the classic linear: exp( r * (P_pos - P_neg) )\n",
    "    # (shifts everything >1, but we’ll still sum up per day)\n",
    "    return math.exp(r * (ppos - pneg))\n",
    "\n",
    "def f6(r, ppos, pneg, pneu):\n",
    "    # r * exp(P_pos - P_neg)  (relevance times exponential of “net sentiment”)\n",
    "    return r * math.exp(ppos - pneg)\n",
    "\n",
    "def f7(r, ppos, pneg, pneu):\n",
    "    # r * logistic( P_pos - P_neg )  to squash net sentiment into (0,1)\n",
    "    z = ppos - pneg\n",
    "    return r * (1 / (1 + math.exp(-z)))\n",
    "\n",
    "def f8(r, ppos, pneg, pneu):\n",
    "    # Polynomial: r * (P_pos - P_neg)^2  so that strong sentiment (pos or neg) is amplified\n",
    "    return r * ((ppos - pneg) ** 2)\n",
    "\n",
    "def f9(r, ppos, pneg, pneu):\n",
    "    # “Hybrid” that penalizes neutral: r * (P_pos - P_neg) * (1 - P_neu)\n",
    "    return r * (ppos - pneg) * (1 - pneu)\n",
    "\n",
    "# You can add more variants here, e.g. f10, f11, etc.\n",
    "\n",
    "f_functions = {\n",
    "    \"f1: r*(P_pos-P_neg)\":      f1,\n",
    "    \"f2: r*P_pos\":              f2,\n",
    "    \"f3: r*(P_pos+0.5P_neu)\":    f3,\n",
    "    \"f4: (P_pos-P_neg)\":         f4,\n",
    "    \"f5: exp(r*(P_pos-P_neg))\":  f5,\n",
    "    \"f6: r*exp(P_pos-P_neg)\":    f6,\n",
    "    \"f7: r*sigmoid(P_pos-P_neg)\":f7,\n",
    "    \"f8: r*(P_pos-P_neg)^2\":     f8,\n",
    "    \"f9: r*(P_pos-P_neg)*(1-P_neu)\": f9\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) Loop over each f, aggregate daily, run time-series CV, print mean accuracy\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, func in f_functions.items():\n",
    "    print(f\"\\n=== Evaluating {name} ===\")\n",
    "    \n",
    "    # 2.a) Apply `func` to all rows of news_df to get an “article_score” column.\n",
    "    #      We assume news_df has columns: ['date', 'relevance', 'P_pos', 'P_neg', 'P_neu'].\n",
    "    news_df[\"article_score\"] = news_df.apply(\n",
    "        lambda row: func(row[\"relevance\"], row[\"P_pos\"], row[\"P_neg\"], row[\"P_neu\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # 2.b) Aggregate per day: DailySentiment = sum of article_score for that date\n",
    "    daily_sentiment = (\n",
    "        news_df\n",
    "        .groupby(\"date\")[\"article_score\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"article_score\": \"daily_sentiment\"})\n",
    "    )\n",
    "    \n",
    "    # 2.c) Merge with ret_df so that daily_sentiment(D) → next_day_return(D)\n",
    "    merged = pd.merge(\n",
    "        daily_sentiment, \n",
    "        ret_df[[\"date\", \"next_day_return\"]],\n",
    "        on=\"date\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # 2.d) Build binary labels: 1 if next_day_return > 0, else 0\n",
    "    merged[\"label\"] = (merged[\"next_day_return\"] > 0).astype(int)\n",
    "    \n",
    "    # 2.e) Prepare X, y\n",
    "    X = merged[[\"daily_sentiment\"]].copy()\n",
    "    y = merged[\"label\"].copy()\n",
    "    \n",
    "    # 2.f) TimeSeriesSplit CV\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold_acc = []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=None, \n",
    "            min_samples_leaf=5, \n",
    "            max_features=\"sqrt\", \n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        fold_acc.append(acc)\n",
    "        \n",
    "        # Optionally, print each fold’s confusion matrix / classification report:\n",
    "        # from sklearn.metrics import confusion_matrix, classification_report\n",
    "        # print(confusion_matrix(y_test, y_pred))\n",
    "        # print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    mean_acc = np.mean(fold_acc)\n",
    "    std_acc  = np.std(fold_acc)\n",
    "    print(f\"→ Mean CV accuracy = {mean_acc:.4f}  (± {std_acc:.4f})\")\n",
    "    results[name] = (mean_acc, std_acc)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3) Summarize all results at once\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n=== Summary of Mean CV Accuracies ===\")\n",
    "for name, (mean_acc, std_acc) in results.items():\n",
    "    print(f\"{name:30s} →  {mean_acc:.4f}  ± {std_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== Summary of Mean CV F1 Scores ===\")\n",
    "for name, func in f_functions.items():\n",
    "\t# Recompute daily sentiment and labels for each function\n",
    "\tnews_df[\"article_score\"] = news_df.apply(\n",
    "\t\tlambda row: func(row[\"relevance\"], row[\"P_pos\"], row[\"P_neg\"], row[\"P_neu\"]),\n",
    "\t\taxis=1\n",
    "\t)\n",
    "\tdaily_sentiment = (\n",
    "\t\tnews_df\n",
    "\t\t.groupby(\"date\")[\"article_score\"]\n",
    "\t\t.sum()\n",
    "\t\t.reset_index()\n",
    "\t\t.rename(columns={\"article_score\": \"daily_sentiment\"})\n",
    "\t)\n",
    "\tmerged = pd.merge(\n",
    "\t\tdaily_sentiment, \n",
    "\t\tret_df[[\"date\", \"next_day_return\"]],\n",
    "\t\ton=\"date\",\n",
    "\t\thow=\"inner\"\n",
    "\t)\n",
    "\tmerged[\"label\"] = (merged[\"next_day_return\"] > 0).astype(int)\n",
    "\tX = merged[[\"daily_sentiment\"]].copy()\n",
    "\ty = merged[\"label\"].copy()\n",
    "\ttscv = TimeSeriesSplit(n_splits=5)\n",
    "\tf1_scores = []\n",
    "\tfor train_idx, test_idx in tscv.split(X):\n",
    "\t\tX_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "\t\ty_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\t\tclf = RandomForestClassifier(\n",
    "\t\t\tn_estimators=200, \n",
    "\t\t\tmax_depth=None, \n",
    "\t\t\tmin_samples_leaf=5, \n",
    "\t\t\tmax_features=\"sqrt\", \n",
    "\t\t\trandom_state=42,\n",
    "\t\t\tn_jobs=-1\n",
    "\t\t)\n",
    "\t\tclf.fit(X_train, y_train)\n",
    "\t\ty_pred = clf.predict(X_test)\n",
    "\t\tf1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\t\tf1_scores.append(f1)\n",
    "\tmean_f1 = np.mean(f1_scores)\n",
    "\tstd_f1 = np.std(f1_scores)\n",
    "\tprint(f\"{name:30s} →  {mean_f1:.4f}  ± {std_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8daa8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the daily sentiment time series for the first function (f1: r*(P_pos-P_neg))\n",
    "# This is the daily_sentiment series for f1, already computed in cell 8\n",
    "\n",
    "# Recompute to ensure correct values for f1\n",
    "def f1(r, ppos, pneg, pneu):\n",
    "    return r * (ppos - pneg)\n",
    "\n",
    "news_df[\"article_score_f1\"] = news_df.apply(\n",
    "    lambda row: f1(row[\"relevance\"], row[\"P_pos\"], row[\"P_neg\"], row[\"P_neu\"]),\n",
    "    axis=1\n",
    ")\n",
    "daily_sentiment_f1 = (\n",
    "    news_df\n",
    "    .groupby(\"date\")[\"article_score_f1\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"article_score_f1\": \"daily_sentiment_f1\"})\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "daily_sentiment_f1.to_csv(\"daily_sentiment_f1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
